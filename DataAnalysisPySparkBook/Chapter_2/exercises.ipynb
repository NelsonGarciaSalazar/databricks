{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5544819-b360-4324-bf20-d2e9a1843adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exercise 2.2\n",
    "Given the following data frame, programmatically count the number of columns that aren’t strings (answer = only one column isn’t a string).\n",
    "createDataFrame() allows you to create a data frame from a variety of sources,such as a pandas data frame or (in this case) a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbca6c93-2cb8-49a5-bf70-56c07167e2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exo2_2_df = spark.createDataFrame(\n",
    "    [\n",
    "        [\"test_1\", \"more test 1\", 10], \n",
    "        [\"test_2\", \"more test 2\", 11], \n",
    "        [\"test_3\", \"more test 3\", 12]\n",
    "    ], \n",
    "    [\"one\", \"two\", \"three\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac3b876d-43bd-4df0-bc86-153ee3758a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exo2_2_df.printSchema()\n",
    "exo2_2_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5f49d0-fef8-48b4-9ea8-5ec312331d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(len([x for x, y in exo2_2_df.dtypes if y != \"string\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afae963-1df4-4125-b3b1-77f10c2b9e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Answer: The column \"Three\" isn't string is long type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a7338c-88bd-4288-b05c-46e6f063c056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exercise 2.3\n",
    "Rewrite the following code snippet, removing the withColumnRenamed method. Which\n",
    "version is clearer and easier to read?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "648b70e7-31d7-4be4-a90c-158e1ae47ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The `length` function returns the number of characters in a string column.\n",
    "\n",
    "# Parameters\n",
    "path_book = \"/Volumes/workspace/dataanalysispysparkbook/bronze_files/1342-0.txt\"\n",
    "\n",
    "from pyspark.sql.functions import col, length\n",
    "\n",
    "exo2_3_df = (spark.read.text(path_book)\n",
    "                        .select(length(col(\"value\")))\n",
    "                        .withColumnRenamed(\"length(value)\", \"number_of_char\")\n",
    "            )\n",
    "exo2_3_df.printSchema()\n",
    "exo2_3_df.show(5, truncate=False)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "157889dd-1109-4cf0-baca-b6e6c2279c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rewrite_df = (spark.read.text(path_book)\n",
    "                        .select(length(col(\"value\")).alias(\"number_of_char\"))\n",
    "             )\n",
    "rewrite_df.printSchema()\n",
    "rewrite_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9868db8-8586-4740-b5b4-1f7be1c2bbfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exercise 2.4\n",
    "Assume a data frame exo2_4_df. The following code block gives an error. What is the\n",
    "problem, and how can you solve it?\n",
    "from pyspark.sql.functions import col, greatest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "442b894c-5b11-4b7d-bc3a-c7606ce376a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exo2_4_df = spark.createDataFrame(\n",
    "    [\n",
    "      [\"key\", 10_000, 20_000]\n",
    "    ], \n",
    "    [\"key\", \"value1\", \"value2\"]\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, greatest\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "exo2_4_df.printSchema()\n",
    "# root\n",
    "# |-- key: string (containsNull = true)\n",
    "# |-- value1: long (containsNull = true)\n",
    "# |-- value2: long (containsNull = true)\n",
    "# `greatest` will return the greatest value of the list of column names,\n",
    "# skipping null value\n",
    "\n",
    "# The following statement will return an error\n",
    "try:\n",
    "    exo2_4_mod = exo2_4_df.select(\n",
    "                        greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\")\n",
    "    ).select(\"key\", \"max_value\")\n",
    "\n",
    "    exo2_4_mod.show(5)\n",
    "except AnalysisException as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f236b24d-d579-4d5c-9f45-c5524f2368c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Añadir la columna con el valor máximo y mantener la columna \"key\"\n",
    "exo2_4_mod = exo2_4_df.select(\n",
    "    col(\"key\"),\n",
    "    greatest(col(\"value1\"), col(\"value2\")).alias(\"max_value\")\n",
    ")\n",
    "\n",
    "exo2_4_mod.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5285f338-f679-436c-bf4c-2fc9c0cd7063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exercise 2.5\n",
    "Let’s take our words_nonull data frame, available in the next listing. You can use the code from the repository (code/Ch02/end_of_chapter.py) in your REPL to get the data frame loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4162d72-97c2-4940-8826-76014a2d351e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, lower, regexp_extract\n",
    "\n",
    "book = spark.read.text(path_book)\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(\n",
    "  regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word\")\n",
    ")\n",
    "words_nonull = words_clean.where(col(\"word\") == \"\")\n",
    "words_nonull.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc9ae21-a3fc-4cfa-8c5d-2963637cca05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "a) Remove all of the occurrences of the word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3185615-680c-4d75-8a30-3393bee60978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "words_clean.filter(col(\"word\") == \"is\").show(5)\n",
    "words_clean_is = words_clean.replace(\"is\", \"\")\n",
    "words_clean_is.filter(col(\"word\") == \"is\").show(5)\n",
    "words_clean_is.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8c4119f-ae07-4672-a571-de15b0ca13d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "b) (Challenge) Using the length function, keep only the words with more than three characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23153d52-2f64-4171-8ae0-94ceb35d4179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, length\n",
    "\n",
    "words_three_letters = words_clean.filter(length(col(\"word\")) >= 3)\n",
    "words_three_letters.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de257f68-5be4-4ab2-9527-b0c742a8c7da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exercise 2.6\n",
    "The where clause takes a Boolean expression over one or many columns to filter the data frame. Beyond the usual Boolean operators (>, <, ==, <=, >=, !=), PySpark provides other functions that return Boolean columns in the pyspark.sql.functions module.\n",
    "\n",
    "A good example is the isin() method (applied on a Column object, like col(…).isin(…)), which takes a list of values as a parameter, and will return only the records where the value in the column equals a member of the list.\n",
    "\n",
    "Let’s say you want to remove the words is, not, the and if from your list of words, using a single where() method on the words_nonull data frame. Write the code to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0880e9c-3899-4ad5-9eb1-df17f1276072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stopwords = [\"no\", \"is\", \"the\", \"if\"]\n",
    "\n",
    "words_filtered = (\n",
    "    words_clean\n",
    "    .filter(col(\"word\").isNotNull())               # Excluir nulls\n",
    "    .filter(col(\"word\") != \"\")                     # Excluir vacías\n",
    "    .filter(~col(\"word\").isin(stopwords))          # Excluir stopwords\n",
    ")\n",
    "words_no_is_not_the_if.show(5)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "265bcfcc-0974-487e-9784-68a57c9a991b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exercise 2.7\n",
    "One of your friends comes to you with the following code. They have no idea why it\n",
    "doesn’t work. Can you diagnose the problem in the try block, explain why it is an\n",
    "error, and provide a fix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe1ff9e-1dff-48b2-a4dd-65e52bd3fcba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "try:\n",
    "    book = spark.read.text(path_book)\n",
    "    # book = book.printSchema()\n",
    "    lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "    words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "    words.show(5)\n",
    "except AnalysisException as err:\n",
    "    print(err)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "exercises",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
