{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "217c7275-9e68-4694-8ee7-b649c9d3246c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "path_book = \"/Volumes/workspace/dataanalysispysparkbook/bronze_files/1342-0.txt\"\n",
    "path_silver = \"/Volumes/workspace/dataanalysispysparkbook/silver/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adfb94ac-83bb-4671-8610-4afcc05ed054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, explode, lower, regexp_extract\n",
    "\n",
    "book = spark.read.text(path_book)\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word\"))\n",
    "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
    "words_nonull.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4cfd57c-44b1-4a4f-90b7-ac60824d2869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results =   (words_nonull.groupby(col(\"word\"))\n",
    "                        .count().orderBy(col(\"count\")\n",
    "                        .desc())\n",
    "            )\n",
    "results.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcefafdd-51f2-4bc8-b8c4-f8a0a611a558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "results = (words_nonull.groupBy(F.col(\"word\"))\n",
    "                        .agg(F.count(F.col(\"word\")).alias(\"count\"))\n",
    "                        .orderBy(F.col(\"count\").desc())\n",
    "            )\n",
    "results.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a61ccbe-daa6-48dc-b2f0-2f0ea51bb80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results.write.csv(\"/Volumes/workspace/dataanalysispysparkbook/silver/simple_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4699672-1d6d-441f-8b00-0aef04769c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/Volumes/workspace/dataanalysispysparkbook/silver/simple_count.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0cb1303-607f-473d-80f6-6768117e7d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results.coalesce(1).write.csv(\"/Volumes/workspace/dataanalysispysparkbook/silver/simple_count_single_partition.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "874a8d25-2d91-4e76-b3b8-0c339afe89a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls dbfs:/Volumes/workspace/dataanalysispysparkbook/silver/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdfcb65f-13b4-4132-8d1e-6158b404d994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Resumen de las instrucciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a10e179-15d1-478e-879e-4084487c16c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "  col,\n",
    "  explode,\n",
    "  lower,\n",
    "  regexp_extract,\n",
    "  split,\n",
    ")\n",
    "\n",
    "book = spark.read.text(path_book) # lee el archivo CSV y crear un data frame book\n",
    "book.show(5, truncate=False)\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\")) # Separa las palabras por espacios\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\")) # Explode las palabras en un array. Cada palabra es un elemento del array\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word\")) # Convierte las palabras a minusculas\n",
    "words_clean = words_lower.select(regexp_extract(col(\"word\"), \"[a-z']*\", 0).alias(\"word\")) # Elimina caracteres de puntuación\n",
    "words_nonull = words_clean.where(col(\"word\") != \"\") # Elimina palabras vacias\n",
    "results = words_nonull.groupby(col(\"word\")).count() # Cuenta las palabras\n",
    "results.orderBy(\"count\", ascending=False).show(5) # Ordena las palabras por cantidad\n",
    "(results.coalesce(1).write.format(\"csv\") # Guarda el resultado en un archivo CSV de una sola partición\n",
    "                          .mode(\"overwrite\")\n",
    "                          .save(f\"{path_silver}/simple_count_single_partition.csv\")\n",
    ")                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6b1943-fce9-4c24-ba63-1c5ec224736b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Resumiendo las instrucciones en una sola operación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b88adf-9041-4b7e-b1d3-05d21d3da936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "results = (\n",
    "  spark.read.text(path_book)\n",
    "            .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "            .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "            .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "            .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "            .where(F.col(\"word\") != \"\")\n",
    "            .groupby(\"word\")\n",
    "            .count()\n",
    "            .orderBy(F.col(\"count\").desc())\n",
    "            # .coalesce(1)\n",
    "            # .write.format(\"csv\"\n",
    ")\n",
    "results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e575bad-2d3a-4882-a143-ae0ec5cc13c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5130211787519622,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Submitting Scaling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
