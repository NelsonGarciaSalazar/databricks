{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe66717d-f152-4014-963f-59ef310e3c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### This chapter covers\n",
    "- Joining two data frames together\n",
    "- Selecting the right type of join for your use case\n",
    "- Grouping data and understanding the\n",
    "- GroupedData transitional object\n",
    "- Breaking the GroupedData with an aggregation method\n",
    "- Filling null values in your data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c1ffc8-6efe-4499-8b1d-1bff34959773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "path_logs = (\"/Volumes/workspace/dataanalysispysparkbook/bronze_files/eda/\")\n",
    "path_reference_tables = (\"/Volumes/workspace/dataanalysispysparkbook/bronze_files/eda/ReferenceTables/\")\n",
    "\n",
    "logs = (spark.read.format(\"csv\")\n",
    "                .option(\"sep\", \"|\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(path_logs + \"BroadcastLogs_2018_Q3_M8_sample.CSV\")\n",
    ")                \n",
    "display(logs.limit(5))\n",
    "\n",
    "log_identifier = (spark.read.format(\"csv\")\n",
    "                .option(\"sep\", \"|\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(path_reference_tables + \"LogIdentifier.csv\")\n",
    ")                \n",
    "display(log_identifier.limit(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb60177a-ec84-4765-aa03-ac74fdc64f65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Estructura de los JOIN\n",
    "[LEFT].join(\n",
    "  [RIGHT],\n",
    "  on=[PREDICATES]\n",
    "  how=[METHOD]\n",
    ")\n",
    "\n",
    "#### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b715e8c-093e-46e9-b6e1-fdb382b79edd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753551649468}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forma Sencilla\n",
    "result = (logs.join(log_identifier, \"LogServiceID\", \"inner\")\n",
    "                .select(logs.LogServiceID, log_identifier.LogServiceID, \"BroadcastLogID\", \"LogIdentifierID\")\n",
    ")\n",
    "\n",
    "# Forma con parametros\n",
    "result_2 = (logs.join(log_identifier, \n",
    "                    on =  \"LogServiceID\", \n",
    "                    how = \"inner\"\n",
    "                )\n",
    "                .select(logs.LogServiceID, log_identifier.LogServiceID, \"BroadcastLogID\", \"LogIdentifierID\")\n",
    ")\n",
    "\n",
    "# Forma con más explicita\n",
    "logs_and_channels_verbose = logs.join(log_identifier, \n",
    "                                        logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"]\n",
    "                                )\n",
    "\n",
    "result.limit(2).display()\n",
    "result_2.limit(2).display()\n",
    "logs_and_channels_verbose.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed950354-0a71-4a12-b9d9-6312c2dd62e7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753551782771}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = (logs.join(log_identifier, \"LogServiceID\", \"inner\")\n",
    "            .groupBy(\"LogServiceID\")\n",
    "            .count()\n",
    "            #.agg(F.count(\"LogServiceID\").alias(\"Count_by_LogServiceID\"))\n",
    "            .orderBy(F.asc(\"LogServiceID\"))\n",
    ")\n",
    "result.limit(5).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afb50eca-fde3-47ff-ba44-26012cf18aef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "LEFT JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c900d5-0873-4744-9908-8945b5cbcf1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = (logs.join(log_identifier, \"LogServiceID\", \"left\")\n",
    "                .select(logs.LogServiceID, log_identifier.LogServiceID, \"BroadcastLogID\", \"LogIdentifierID\")\n",
    ")          \n",
    "result.limit(5).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44dccd93-8f1b-4467-8448-eaee78bcfb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Varios condicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e2a63a-b613-4d9e-965a-1df710be3be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DataFrame 1\n",
    "data1 = [\n",
    "    (1, \"Ana\", 2022),\n",
    "    (2, \"Luis\", 2023),\n",
    "    (3, \"Carlos\", 2022),\n",
    "    (4, \"Julia\", 2024)\n",
    "]\n",
    "df1 = spark.createDataFrame(data1, [\"id\", \"nombre\", \"anio\"])\n",
    "\n",
    "# DataFrame 2\n",
    "data2 = [\n",
    "    (1, \"Lima\", 2022),\n",
    "    (2, \"Arequipa\", 2022),\n",
    "    (3, \"Trujillo\", 2023),\n",
    "    (4, \"Cusco\", 2024)\n",
    "]\n",
    "df2 = spark.createDataFrame(data2, [\"id\", \"ciudad\", \"anio\"])\n",
    "\n",
    "# forma usando tabla.columna\n",
    "df_join = df1.join(\n",
    "    df2,\n",
    "    ((df1.id == df2.id) & (df1.anio == df2.anio)),\n",
    "    \"inner\"\n",
    ")\n",
    "df_join.show(2)\n",
    "\n",
    "# Forma con el nombre de las tablas\n",
    "df_join_1 = df1.join(\n",
    "    df2,\n",
    "    (df1[\"id\"] == df2[\"id\"]) & (df1[\"anio\"] == df2[\"anio\"]),\n",
    "    how=\"inner\"\n",
    ")\n",
    "df_join_1.show(2)\n",
    "\n",
    "# Usando Alias para las tablas\n",
    "df1_alias = df1.alias(\"a\")\n",
    "df2_alias = df2.alias(\"b\")\n",
    "\n",
    "df_join_alias = df1_alias.join(\n",
    "    df2_alias,\n",
    "    (F.col(\"a.id\") == F.col(\"b.id\")) & (F.col(\"a.anio\") == F.col(\"b.anio\")),\n",
    "    \"inner\"\n",
    ")\n",
    "df_join_alias.show(2)\n",
    "\n",
    "# Operador OR\n",
    "df_join_2 = df1.join(\n",
    "    df2,\n",
    "    (df1[\"id\"] == df2[\"id\"]) | (df1[\"anio\"] == df2[\"anio\"]),\n",
    "    how=\"inner\"\n",
    ")\n",
    "df_join_2.show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d65a054-a181-4ba8-9828-a0f888f96d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Columns with the same name, para evitar la ambiguedad se debe colar de predicado la tabla a la que pertenece. Tabla.Columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f571f3-97fd-47fd-ac6d-47bb5a9c2e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.errors import AnalysisException\n",
    "\n",
    "try:\n",
    "    logs_and_channels_verbose.select(\"LogServiceID\")\n",
    "except AnalysisException as err:\n",
    "    print(err)\n",
    "\n",
    "logs_and_channels = logs.join(log_identifier, \"LogServiceID\")\n",
    "logs_and_channels.limit(2).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37400707-a08e-4315-81cc-7ccbbbd4a16d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Hacer JOIN con más de una tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c636f45-1e10-4755-b695-4617e6cc35ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cd_category = spark.read.csv(\n",
    "  path_reference_tables + \"CD_Category.csv\",\n",
    "  sep=\"|\",\n",
    "  header=True,\n",
    "  inferSchema=True,\n",
    ").select(\n",
    "  \"CategoryID\",\n",
    "  \"CategoryCD\",\n",
    "  F.col(\"EnglishDescription\").alias(\"Category_Description\"),\n",
    ")\n",
    "\n",
    "cd_program_class = spark.read.csv(\n",
    "  path_reference_tables + \"CD_ProgramClass.csv\",\n",
    "  sep=\"|\",\n",
    "  header=True,\n",
    "  inferSchema=True,\n",
    ").select(\n",
    "  \"ProgramClassID\",\n",
    "  \"ProgramClassCD\",\n",
    "  F.col(\"EnglishDescription\").alias(\"ProgramClass_Description\"),\n",
    ")\n",
    "\n",
    "full_log = (logs_and_channels.join(cd_category, \"CategoryID\", how=\"left\")\n",
    "                             .join(cd_program_class, \"ProgramClassID\", how=\"left\")\n",
    ")\n",
    "full_log.limit(2).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da5c045-0d47-416f-bee3-f36d118683e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exercise 5.1\n",
    "Assume two tables, left and right, each containing a column named my_column.  \n",
    "What is the result of this code?  \n",
    "one = left.join(right, how=\"left_semi\", on=\"my_column\").  \n",
    "two = left.join(right, how=\"left_anti\", on=\"my_column\").  \n",
    "one.union(two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7b4384-eca1-45e5-920a-cc7421164f34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exercise 5.2\n",
    "Assume two data frames, red and blue. Which is the appropriate join to use in red.join(blue, …) if you want to join red and blue and keep all the records satisfying the predicate?\n",
    "1. Left\n",
    "2. Right\n",
    "3. Inner \"Correct\"\n",
    "4. Theta\n",
    "5. Cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2dca348-c399-4c3f-9781-da76a9fe67c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exercise 5.3\n",
    "Assume two data frames, red and blue. Which is the appropriate join to use in red.join(blue, …) if you want to join red and blue and keep all the records satisfying the predicate and the records in the blue table?.\n",
    "1. Left\n",
    "2. Right \"Correct\"\n",
    "3. Inner\n",
    "4. Theta\n",
    "5. Cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8374e48-531a-4b4e-8a76-c8ba82bdb8ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Agrupando por varios campos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27502365-2727-48a7-bef2-4051596ed829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Parseamos la columna ISO 8601 como timestamp real\n",
    "full_log = full_log.withColumn(\"Duration_ts\", F.to_timestamp(\"Duration\"))\n",
    "\n",
    "# 2. Extraemos solo la hora, minuto, segundo de ese timestamp\n",
    "full_log = full_log.withColumn(\"hours\", F.hour(\"Duration_ts\"))\n",
    "full_log = full_log.withColumn(\"minutes\", F.minute(\"Duration_ts\"))\n",
    "full_log = full_log.withColumn(\"seconds\", F.second(\"Duration_ts\"))\n",
    "\n",
    "# 3. Calculamos total en segundos\n",
    "full_log = full_log.withColumn(\n",
    "    \"Duration_seconds\",\n",
    "    F.col(\"hours\") * 3600 + F.col(\"minutes\") * 60 + F.col(\"seconds\")\n",
    ")\n",
    "\n",
    "# 4. Agrupamos y sumamos como antes\n",
    "agg_df = full_log.groupBy(\"ProgramClassCD\", \"ProgramClass_Description\") \\\n",
    "    .agg(F.sum(\"Duration_seconds\").alias(\"duration_total\")) \\\n",
    "    .orderBy(F.desc(\"duration_total\"))\n",
    "\n",
    "# 5. Mostramos el resultado\n",
    "agg_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d862a4-e463-4983-bcbb-9ae724733e37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "answer = (\n",
    "    full_log.groupBy(\"LogIdentifierID\")\n",
    "    .agg(\n",
    "        F.sum(\n",
    "            F.when(\n",
    "                F.trim(F.col(\"ProgramClassCD\")).isin(\n",
    "                    [\"COM\", \"PRC\", \"PGI\", \"PRO\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]\n",
    "                ),\n",
    "                F.col(\"Duration_seconds\"),\n",
    "            ).otherwise(0)\n",
    "        ).alias(\"duration_commercial\"),\n",
    "        F.sum(\"Duration_seconds\").alias(\"duration_total\"),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"commercial_ratio\",\n",
    "        F.when(F.col(\"duration_total\") != 0,\n",
    "               F.col(\"duration_commercial\") / F.col(\"duration_total\")\n",
    "        ).otherwise(None)  # o 0.0 si prefieres\n",
    "    )\n",
    ")\n",
    "\n",
    "answer.orderBy(\"commercial_ratio\", ascending=False).show(5, False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "joining_grouping",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
