{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c05ced6-7714-4fe4-9379-410d659594e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exercise 6.1\n",
    "For this solution, I create a dictionary copy of the JSON document that I then dump using the json.dump function. Because spark.read.json can only read files, we use a neat trick where we create an RDD (see chapter 8) that can be read via our spark.read.json (see http://mng.bz/g41E for more information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd91b6b8-b043-4509-b271-8a7792e4a9c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "exo6_1_json = {\"name\": \"Sample name\",\n",
    "                \"keywords\": [\"PySpark\", \"Python\", \"Data\"]}\n",
    "\n",
    "exo6_1_json = json.dumps(exo6_1_json)\n",
    "pprint.pprint(exo6_1_json)\n",
    "# '{\"name\": \"Sample name\", \"keywords\": [\"PySpark\", \"Python\", \"Data\"]}'\n",
    "\n",
    "#sol6_1 = spark.read.json(spark.sparkContext.parallelize([exo6_1_json]))\n",
    "#sol6_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80f2d1dc-3502-477e-b653-127091b4ad62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exercise 6.2\n",
    "Although we have a number in our list/array of keywords, PySpark will default to the lowest common denominator and create an array of strings. The answer is the same as exercise 6.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c3fd7ee-ded9-4585-8a75-e63cd8a7735f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "exo6_2_json = {\n",
    "  \"name\": \"Sample name\",\n",
    "  \"keywords\": [\"PySpark\", 3.2, \"Data\"],\n",
    "}\n",
    "exo6_2_json = json.dumps(exo6_2_json)\n",
    "pprint.pprint(exo6_2_json)\n",
    "\n",
    "#sol6_2 = spark.read.json(spark.sparkContext.parallelize([exo6_2_json]))\n",
    "#sol6_2.printSchema()\n",
    "#sol6_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e6c4eb3-90c3-444b-bb11-e246de3d6c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Exercise_6",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
